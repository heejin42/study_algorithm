# 1. 선형 회귀(Linear Regression)
대표적인 머신러닝 데이터 분석의 목적 중 하나는 실제 데이터를 바탕으로 모델을 생성해서 다른 입력 값에 대해서도 발생할 아웃풋을 예측하는 것이다. 여기서 가장 직관적이고 간단한 모델은 선으로 선을 표현한 모델이 선형 회귀 모델이다.    
 
### MSE(Mean Square Error) = 평균 제곱 오차
데이터를 놓고 선을 긋는 다는 건 대충 어림잡을 수 있는 정도를 찾는다는 것인데, 그렇기 때문에 실제 데이터와 차이가 존재한다. 이 차이를 손실(Loss)이라고 부르며 음과 양이 될 수 있으므로 모든 손실에 제곱을 헤준 평균 제곱 오차라는 손실을 많이 사용한다.   
선형 회귀 모델의 목표는 모든 데이터로부터 나오는 오차의 평균을 최소화할 수 있는 최적의 기울기와 절편 값을 찾는 것이다. 그렇다면 어떻게 손실을 최소화할 수 있을까?

### 경사하강법(Gradient Descent)
손실을 함수로 나타내면 아마 아래로 볼록한 모양이 될 것이다. 그럼 이 모델은 임의로 파라미터를 정한 다음 미분을 통해 기울기를 구해 손실을 조금씩 줄여가는 방법으로 최적의 파라미터를 찾아낸다. 이 방법을 경사하강법이라고 부른다. 


### 수렴(Convergence)
위에서 말한대로 선형 회귀 분석을 수행하면 기울기와 절편을 계속 변경해가며 최적의 값을 찾을 것이다. 하지만 이 작업을 언제까지 할 지 모른다. 선형회귀 모델은 어느정도 적당한 최적의 값으로 수렴하도록 그 기준을 정한다. 여기서 우리는 관여하지 않지만 학습률이라는 걸 정해줘야 한다. 

### 학습룰(Learning Rate)
학습률이 너무 크면 최적의 값을 제대로 찾지 못하고 듬성듬성 보게 된다. 하지만 너무 작게 설정한다면 최적의 값으로 수렴할 때까지 너무 오래 걸리게 된다. 그러므로 효율적으로 파라미터를 조정하면서도 최적의 값을 찾을 수 있는 수준으로 적절한 학습률을 지정해줘야 한다.    

### 코드
그럼 sklearn의 LinearRegression을 사용해 선형 회귀 예측 모델을 만들어보겠다. 


# 2. 로지스틱 회귀(Logistic Regression)
# 3. K-최근접 이웃(KNN)
# 4. 나이브 베이즈(Naive Bayes)
# 5. 결정 트리(Decision Tree)
# 6. 랜덤 포레스트(Random Forest)
# 7. XG부스트(XGBoost)
# 8. 라이트GBM(LightGBM)
# 9. K-평균 군집화(K Means Clustering)
# 10. 주성분 분석(PCA)