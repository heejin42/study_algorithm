# 자연어처리란?
자연어(natural language)란 우리가 일상 생활에서 사용하는 언어를 말합니다. 자연어 처리(natural language processing)란 이러한 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일을 말한다.

자연어 처리는 음성 인식, 내용 요약, 번역, 사용자의 감성 분석, 텍스트 분류 작업(스팸 메일 분류, 뉴스 기사 카테고리 분류), 질의 응답 시스템, 챗봇과 같은 곳에서 사용되는 분야로 최근 딥러닝의 성능이 좋아지면서 많이 적용되고 있다.
   
----------------------------------
   
## 1. 텍스트 전처리 - NLTK, KoNLPy
엔엘티케이(NLTK) - 자연어 처리를 위한 파이썬 패키지
코엔엘파이(KoNLPy) - 한국어 자연어 처리를 위한 형태소 분석기 패키지
### 토큰화(Tokenization)
주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 한다. 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의한다. 그 단위는 단어가 될 수도 있고, 문장이 될 수도 있다.   
토큰화를 할 때는 마침표나 컴마, 느낌표 등의 구두점을 어떻게 처리할지 고민을 하게 된다. 예를 들어 Don't의 단어를 구두점을 없애는 방식으로 처리한다면 don과 t가 될 것이고, 구두점을 보존한다면 don't가 될 것이다. 즉, 구두점이나 특수 문자를 단순 제외하는 것이 토큰화 작업이라고 생각해서는 안된다. 또한 줄임말이나 하나의 의미를 가진 단어 안에 띄어쓰기가 있는 경우가 있을 수 있다. 이러한 여러 가지 가능성을 고려하여 가장 적합하게 토큰화할 수 있도록 적절한 함수와 알고리즘을 사용해야 한다. 대표적인 토크나이저 몇가지를 알아보겠다. 

- nltk의 word_tokenize
- nltk의 WordPunctTokenier
- tensorflow의 text_to_word_sequence
- okt: 한국어 데이터에 대해 단어를 분리하며 각 단어의 품사를 태깅한다. 명사만 따로 추출할 수도 있다. 
- Kkma: 한국어 데이터에 대해 sentense()는 문장을 분리하며, nouns()는 명사를 분리한다. morphs()는 형태소를 분석한다. 

### 전처리 
1. 같은 의미의 단어 통합, 대소문자 통합
2. 불용어, 길이가 짧거나 등장 빈도가 적은 단어 제거 (노이즈 데이터)
3. html 태그와 같이 노이즈 데이터의 특징이 있다면 정규표현식으로 처리
### 패딩
병렬 연산을 위해 문장이나 단어의 길이를 임의로 동일하게 맞춰주는 작업
### 원-핫 인코딩
텍스트의 모든 단어를 중복을 허용하지 않고 모아서 단어 집합을 만든다. 단어 집합에 고유한 정수를 부여하는 정수 인코딩을 거친 후, 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식의 원-핫 인코딩을 한다. 
   
----------------------------------
   
## 2. 언어 모델 (Languagel Model, LM)
언어 모델(Language Model, LM)은 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델, 즉 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델이다. 통계적 언어 모델과 신경망 언어 모델이 있는데 통계적 언어 모델은 전통적인 언어모델이며, 최근 핫한 자연어 처리의 기술인 GPT나 BERT는 인공 신경망 언어 모델의 개념을 사용하여 만들어졌다.   
언어 모델에 -ing를 붙인 언어 모델링(Language Modeling)은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 말한다. 즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일은 언어 모델링이라고 한다.

### 통계적 언어 모델(Statistical Language Model, SLM)
조건부 확률은 다음과 같은 관계를 갖는다. 
- P(A,B) = P(A)*P(B|A)
- P(A,B,C,D) = P(A)*P(B|A)*P(C|A,B)*P(D|A,B,C)
조건부 확률의 연쇄법칙을 알 수 있다. 이와 같은 조건부 확률의 정의를 이용해 문장의 확률을 구한다면 문장의 각 단어에 대한 예측 확률들을 곱하면 된다.   
   
----------------------------------
  
## 3. 카운트 기반의 단어 표현
자연어 처리에서 텍스트를 표현하는 방법 중 정보 검색과 텍스트 마이닝 분야에는 카운트 기반의 방식이 많이 사용된다. 단어 등의 기준으로 카운트하여 수치화하고 나면 통계적인 접근 방법들을 사용하여 단어의 중요도를 계산하거나 검색 결과의 순위를 결정하고, 문서들 간의 유사도를 구하는 등의 작업을 할 수 있다.    
크게 카운트 기반의 단어 표현은 국소 표현과 분산 표현으로 나누어 진다. 국소 표현은 해당 단어 그 자체만 보고 값을 맵핑하여 단어를 표현하는 방법이고, 분산 표현 방법은 그 단어를 표현할 때 주변을 참고하는 방식이다. 예를 들어 'puppy', 'cute', 'lovely' 세개 단어가 있을 때, 각 단어에 1,2,3번의 숫자를 단순이 맵핑한다면 국소 표현에 해당할 것이다. 만약 puppy라는 단어 근처에 주로 cute, lovely 등의 단어가 자주 등장하므로 이 단어들로 정의한다면 분산 표현이 될 것이다. 여기서 해볼 내용들은 모두 국소 표현에 해당하고 뒤에 나올 워드 임베딩, 워드투벡터 등의 방식은 분산 표현에 사용한다고 이해하면 된다.

### Bag of Words(BoW)
ㅇㅇ

### 문서 단어 행렬

### TF-IDF

----------------------------------
## 4. 워드 임베딩, word2vec
https://wikidocs.net/22644


----------------------------------
## 5. 머신러닝 
### 벡터 유사도 
https://wikidocs.net/24602

### 텍스트 분류
https://wikidocs.net/22891


### 감정 분석

----------------------------------
## 6. 딥러닝
### 챗봇
### BERT 실습